{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanttouchthis/gpt-4chan-colab/blob/main/gpt-4chan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_sMF3p52I8K"
      },
      "source": [
        "You can get the model file(s) from https://archive.org/details/gpt4chan_model_float16\n",
        "and the config file from https://github.com/Aspie96/gpt-4chan-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhM2vaVH3fuo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6W_elwT3wud"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "GLWGa7Z3kwaL"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPast,\n",
        "    CausalLMOutputWithPast,\n",
        ")\n",
        "from transformers import GPTJModel, GPTJForCausalLM, AutoTokenizer\n",
        "from torch.nn import CrossEntropyLoss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def forward(\n",
        "    self,\n",
        "    input_ids: Optional[torch.LongTensor] = None,\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "    attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    token_type_ids: Optional[torch.LongTensor] = None,\n",
        "    position_ids: Optional[torch.LongTensor] = None,\n",
        "    head_mask: Optional[torch.FloatTensor] = None,\n",
        "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "    use_cache: Optional[bool] = None,\n",
        "    output_attentions: Optional[bool] = None,\n",
        "    output_hidden_states: Optional[bool] = None,\n",
        "    return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
        "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "    output_hidden_states = (\n",
        "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "    )\n",
        "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "    if input_ids is not None and inputs_embeds is not None:\n",
        "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "    elif input_ids is not None:\n",
        "        input_shape = input_ids.size()\n",
        "        input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        batch_size = input_ids.shape[0]\n",
        "    elif inputs_embeds is not None:\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "        batch_size = inputs_embeds.shape[0]\n",
        "    else:\n",
        "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
        "\n",
        "    if position_ids is not None:\n",
        "        position_ids = position_ids.view(-1, input_shape[-1])\n",
        "\n",
        "    if past_key_values is None:\n",
        "        past_length = 0\n",
        "        past_key_values = tuple([None] * len(self.h))\n",
        "    else:\n",
        "        past_length = past_key_values[0][0].size(-2)\n",
        "\n",
        "    if position_ids is None:\n",
        "        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
        "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
        "\n",
        "    # Attention mask.\n",
        "    if attention_mask is not None:\n",
        "        if batch_size <= 0:\n",
        "            raise ValueError(\"batch_size has to be defined and > 0\")\n",
        "        attention_mask = attention_mask.view(batch_size, -1)\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        attention_mask = attention_mask[:, None, None, :]\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
        "        attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
        "\n",
        "    # Prepare head mask if needed\n",
        "    # 1.0 in head_mask indicate we keep the head\n",
        "    # attention_probs has shape bsz x num_attention_heads x N x N\n",
        "    # head_mask has shape n_layer x batch x num_attention_heads x N x N\n",
        "    head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
        "    if inputs_embeds is None:\n",
        "        inputs_embeds = self.wte(input_ids)\n",
        "    \n",
        "    hidden_states = inputs_embeds.cpu()\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        token_type_embeds = self.wte(token_type_ids)\n",
        "        hidden_states = hidden_states + token_type_embeds\n",
        "    hidden_states = self.drop(hidden_states)\n",
        "\n",
        "    output_shape = input_shape + (hidden_states.size(-1),)\n",
        "\n",
        "    presents = () if use_cache else None\n",
        "    all_self_attentions = () if output_attentions else None\n",
        "    all_hidden_states = () if output_hidden_states else None\n",
        "    self.ln_f.cuda().half()\n",
        "    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "        block.cuda()\n",
        "        outputs = block(\n",
        "            hidden_states.cuda(),\n",
        "            layer_past=layer_past,\n",
        "            attention_mask=attention_mask.cuda(),\n",
        "            head_mask=head_mask[i],\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        block.cpu()\n",
        "        hidden_states = outputs[0]\n",
        "        if use_cache is True:\n",
        "            presents = presents + (outputs[1],)\n",
        "\n",
        "        if output_attentions:\n",
        "            all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
        "\n",
        "        # Model Parallel: If it's the last layer for that device, put things on the next device\n",
        "        if self.model_parallel:\n",
        "            for k, v in self.device_map.items():\n",
        "                if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
        "                    hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
        "    hidden_states = self.ln_f(hidden_states.half()).cpu().float()\n",
        "\n",
        "    hidden_states = hidden_states.view(output_shape)\n",
        "    # Add last hidden state\n",
        "    if output_hidden_states:\n",
        "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "    if not return_dict:\n",
        "        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
        "\n",
        "    return BaseModelOutputWithPast(\n",
        "        last_hidden_state=hidden_states.half(),\n",
        "        past_key_values=presents,\n",
        "        hidden_states=all_hidden_states,\n",
        "        attentions=all_self_attentions,\n",
        "    )\n",
        "\n",
        "GPTJModel.forward = forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def forward2(\n",
        "    self,\n",
        "    input_ids: Optional[torch.LongTensor] = None,\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "    attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    token_type_ids: Optional[torch.LongTensor] = None,\n",
        "    position_ids: Optional[torch.LongTensor] = None,\n",
        "    head_mask: Optional[torch.FloatTensor] = None,\n",
        "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "    labels: Optional[torch.LongTensor] = None,\n",
        "    use_cache: Optional[bool] = None,\n",
        "    output_attentions: Optional[bool] = None,\n",
        "    output_hidden_states: Optional[bool] = None,\n",
        "    return_dict: Optional[bool] = None,\n",
        ") -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "    r\"\"\"\n",
        "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
        "        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
        "        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
        "    \"\"\"\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    transformer_outputs = self.transformer(\n",
        "        input_ids,\n",
        "        past_key_values=past_key_values,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "        position_ids=position_ids,\n",
        "        head_mask=head_mask,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        use_cache=use_cache,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "    )\n",
        "    hidden_states = transformer_outputs[0]\n",
        "\n",
        "    # Set device for model parallelism\n",
        "    if self.model_parallel:\n",
        "        torch.cuda.set_device(self.transformer.first_device)\n",
        "        hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
        "\n",
        "    # make sure sampling in fp16 works correctly and\n",
        "    # compute loss in fp32 to match with mesh-tf version\n",
        "    # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n",
        "    self.lm_head.cuda()\n",
        "    lm_logits = self.lm_head(hidden_states.cuda()).to(torch.float32).cpu()\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "        # Shift so that tokens < n predict n\n",
        "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        # Flatten the tokens\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        loss = loss.to(hidden_states.dtype)\n",
        "\n",
        "    if not return_dict:\n",
        "        output = (lm_logits,) + transformer_outputs[1:]\n",
        "        return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "    return CausalLMOutputWithPast(\n",
        "        loss=loss,\n",
        "        logits=lm_logits,\n",
        "        past_key_values=transformer_outputs.past_key_values,\n",
        "        hidden_states=transformer_outputs.hidden_states,\n",
        "        attentions=transformer_outputs.attentions,\n",
        "    )\n",
        "GPTJForCausalLM.forward = forward2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"./model/\" \n",
        "\n",
        "model = GPTJForCausalLM.from_pretrained(\n",
        "    model_path, revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True, local_files_only=True\n",
        ")\n",
        "model.eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "uBLdINiR3jK_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "def generate(prompt, **kwargs):\n",
        "    torch.cuda.empty_cache()\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    gen_tokens = model.generate(input_ids, **kwargs)\n",
        "    return tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "generation_kwargs ={\n",
        "    \"do_sample\":True,\n",
        "    \"temperature\":0.8,\n",
        "    \"top_p\":0.9,\n",
        "    \"max_length\":256\n",
        "}\n",
        "\n",
        "prompt = (\n",
        "    \"--- 152558911\\n\"\n",
        "    \">be me\\n\"\n",
        ")\n",
        "\n",
        "generated_text = generate(prompt, **generation_kwargs)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyP66NG9nGwpyNllJRu5i2Qt",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "4chan.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('clip')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e925216d1263a41745c0ed264d9caafa877ab94bbeda3509aaa93872cda3b164"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
