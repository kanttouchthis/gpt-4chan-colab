{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanttouchthis/gpt-4chan-colab/blob/main/gpt-4chan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_sMF3p52I8K"
      },
      "source": [
        "You can get the model file(s) from https://archive.org/details/gpt4chan_model_float16\n",
        "and the config file from https://github.com/Aspie96/gpt-4chan-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GLWGa7Z3kwaL"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPast,\n",
        "    CausalLMOutputWithPast,\n",
        ")\n",
        "from transformers import GPTJModel, GPTJForCausalLM, AutoTokenizer\n",
        "from torch.nn import CrossEntropyLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def forward(\n",
        "    self,\n",
        "    input_ids: Optional[torch.LongTensor] = None,\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "    attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    token_type_ids: Optional[torch.LongTensor] = None,\n",
        "    position_ids: Optional[torch.LongTensor] = None,\n",
        "    head_mask: Optional[torch.FloatTensor] = None,\n",
        "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "    use_cache: Optional[bool] = None,\n",
        "    output_attentions: Optional[bool] = None,\n",
        "    output_hidden_states: Optional[bool] = None,\n",
        "    return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
        "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "    output_hidden_states = (\n",
        "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "    )\n",
        "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "    if input_ids is not None and inputs_embeds is not None:\n",
        "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "    elif input_ids is not None:\n",
        "        input_shape = input_ids.size()\n",
        "        input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        batch_size = input_ids.shape[0]\n",
        "    elif inputs_embeds is not None:\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "        batch_size = inputs_embeds.shape[0]\n",
        "    else:\n",
        "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
        "\n",
        "    if position_ids is not None:\n",
        "        position_ids = position_ids.view(-1, input_shape[-1])\n",
        "\n",
        "    if past_key_values is None:\n",
        "        past_length = 0\n",
        "        past_key_values = tuple([None] * len(self.h))\n",
        "    else:\n",
        "        past_length = past_key_values[0][0].size(-2)\n",
        "\n",
        "    if position_ids is None:\n",
        "        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
        "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
        "\n",
        "    # Attention mask.\n",
        "    if attention_mask is not None:\n",
        "        if batch_size <= 0:\n",
        "            raise ValueError(\"batch_size has to be defined and > 0\")\n",
        "        attention_mask = attention_mask.view(batch_size, -1)\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        attention_mask = attention_mask[:, None, None, :]\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
        "        attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
        "\n",
        "    # Prepare head mask if needed\n",
        "    # 1.0 in head_mask indicate we keep the head\n",
        "    # attention_probs has shape bsz x num_attention_heads x N x N\n",
        "    # head_mask has shape n_layer x batch x num_attention_heads x N x N\n",
        "    head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
        "    if inputs_embeds is None:\n",
        "        inputs_embeds = self.wte(input_ids)\n",
        "    \n",
        "    hidden_states = inputs_embeds.cpu()\n",
        "\n",
        "    if token_type_ids is not None:\n",
        "        token_type_embeds = self.wte(token_type_ids)\n",
        "        hidden_states = hidden_states + token_type_embeds\n",
        "    hidden_states = self.drop(hidden_states)\n",
        "\n",
        "    output_shape = input_shape + (hidden_states.size(-1),)\n",
        "\n",
        "    presents = () if use_cache else None\n",
        "    all_self_attentions = () if output_attentions else None\n",
        "    all_hidden_states = () if output_hidden_states else None\n",
        "    self.ln_f.cuda().half()\n",
        "    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "        block.cuda()\n",
        "        outputs = block(\n",
        "            hidden_states.cuda(),\n",
        "            layer_past=layer_past,\n",
        "            attention_mask=attention_mask.cuda(),\n",
        "            head_mask=head_mask[i],\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        block.cpu()\n",
        "        hidden_states = outputs[0]\n",
        "        if use_cache is True:\n",
        "            presents = presents + (outputs[1],)\n",
        "\n",
        "        if output_attentions:\n",
        "            all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
        "\n",
        "        # Model Parallel: If it's the last layer for that device, put things on the next device\n",
        "        if self.model_parallel:\n",
        "            for k, v in self.device_map.items():\n",
        "                if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
        "                    hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
        "    hidden_states = self.ln_f(hidden_states.half()).cpu().float()\n",
        "\n",
        "    hidden_states = hidden_states.view(output_shape)\n",
        "    # Add last hidden state\n",
        "    if output_hidden_states:\n",
        "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "    if not return_dict:\n",
        "        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
        "\n",
        "    return BaseModelOutputWithPast(\n",
        "        last_hidden_state=hidden_states.half(),\n",
        "        past_key_values=presents,\n",
        "        hidden_states=all_hidden_states,\n",
        "        attentions=all_self_attentions,\n",
        "    )\n",
        "\n",
        "GPTJModel.forward = forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def forward2(\n",
        "    self,\n",
        "    input_ids: Optional[torch.LongTensor] = None,\n",
        "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "    attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    token_type_ids: Optional[torch.LongTensor] = None,\n",
        "    position_ids: Optional[torch.LongTensor] = None,\n",
        "    head_mask: Optional[torch.FloatTensor] = None,\n",
        "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "    labels: Optional[torch.LongTensor] = None,\n",
        "    use_cache: Optional[bool] = None,\n",
        "    output_attentions: Optional[bool] = None,\n",
        "    output_hidden_states: Optional[bool] = None,\n",
        "    return_dict: Optional[bool] = None,\n",
        ") -> Union[Tuple, CausalLMOutputWithPast]:\n",
        "    r\"\"\"\n",
        "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
        "        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
        "        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
        "        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
        "    \"\"\"\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    transformer_outputs = self.transformer(\n",
        "        input_ids,\n",
        "        past_key_values=past_key_values,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "        position_ids=position_ids,\n",
        "        head_mask=head_mask,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        use_cache=use_cache,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "    )\n",
        "    hidden_states = transformer_outputs[0]\n",
        "\n",
        "    # Set device for model parallelism\n",
        "    if self.model_parallel:\n",
        "        torch.cuda.set_device(self.transformer.first_device)\n",
        "        hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
        "\n",
        "    # make sure sampling in fp16 works correctly and\n",
        "    # compute loss in fp32 to match with mesh-tf version\n",
        "    # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n",
        "    self.lm_head.cuda()\n",
        "    lm_logits = self.lm_head(hidden_states.cuda()).to(torch.float32).cpu()\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "        # Shift so that tokens < n predict n\n",
        "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        # Flatten the tokens\n",
        "        loss_fct = CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        loss = loss.to(hidden_states.dtype)\n",
        "\n",
        "    if not return_dict:\n",
        "        output = (lm_logits,) + transformer_outputs[1:]\n",
        "        return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "    return CausalLMOutputWithPast(\n",
        "        loss=loss,\n",
        "        logits=lm_logits,\n",
        "        past_key_values=transformer_outputs.past_key_values,\n",
        "        hidden_states=transformer_outputs.hidden_states,\n",
        "        attentions=transformer_outputs.attentions,\n",
        "    )\n",
        "GPTJForCausalLM.forward = forward2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\TheCa\\Code\\python\\gpt-4chan-colab\\gpt-4chan_low.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TheCa/Code/python/gpt-4chan-colab/gpt-4chan_low.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./model/\u001b[39m\u001b[39m\"\u001b[39m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/TheCa/Code/python/gpt-4chan-colab/gpt-4chan_low.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m GPTJForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TheCa/Code/python/gpt-4chan-colab/gpt-4chan_low.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model_path, revision\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfloat16\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16, low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, local_files_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TheCa/Code/python/gpt-4chan-colab/gpt-4chan_low.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TheCa/Code/python/gpt-4chan-colab/gpt-4chan_low.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/TheCa/Code/python/gpt-4chan-colab/gpt-4chan_low.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mEleutherAI/gpt-j-6B\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\TheCa\\.conda\\envs\\clip\\lib\\site-packages\\transformers\\modeling_utils.py:1364\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1363\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1364\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(resolved_archive_file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   1365\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1366\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\TheCa\\.conda\\envs\\clip\\lib\\site-packages\\torch\\serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[0;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
            "File \u001b[1;32mc:\\Users\\TheCa\\.conda\\envs\\clip\\lib\\site-packages\\torch\\serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1044\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1045\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1046\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1048\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1050\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\TheCa\\.conda\\envs\\clip\\lib\\site-packages\\torch\\serialization.py:1016\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[0;32m   1015\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1016\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1018\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
            "File \u001b[1;32mc:\\Users\\TheCa\\.conda\\envs\\clip\\lib\\site-packages\\torch\\serialization.py:997\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[0;32m    995\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 997\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[0;32m    998\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m    999\u001b[0m     \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m   1001\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1002\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_path = \"./model/\" \n",
        "\n",
        "model = GPTJForCausalLM.from_pretrained(\n",
        "    model_path, revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True, local_files_only=True\n",
        ")\n",
        "model.eval()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBLdINiR3jK_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "def generate(prompt, **kwargs):\n",
        "    torch.cuda.empty_cache()\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    gen_tokens = model.generate(input_ids, **kwargs)\n",
        "    return tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "generation_kwargs ={\n",
        "    \"do_sample\":True,\n",
        "    \"temperature\":0.8,\n",
        "    \"top_p\":0.9,\n",
        "    \"max_length\":256\n",
        "}\n",
        "\n",
        "prompt = (\n",
        "    \"--- 152558911\\n\"\n",
        "    \">be me\\n\"\n",
        ")\n",
        "\n",
        "generated_text = generate(prompt, **generation_kwargs)\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyP66NG9nGwpyNllJRu5i2Qt",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "4chan.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('clip')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e925216d1263a41745c0ed264d9caafa877ab94bbeda3509aaa93872cda3b164"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
